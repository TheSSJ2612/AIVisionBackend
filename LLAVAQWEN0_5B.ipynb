{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformers import (\n",
    "\n",
    "    BitsAndBytesConfig,\n",
    "\n",
    "    LlavaOnevisionForConditionalGeneration,\n",
    "\n",
    "    LlavaOnevisionProcessor,\n",
    "\n",
    ")\n",
    "import torch\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "\n",
    "    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "\n",
    "    \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\",\n",
    "\n",
    "    torch_dtype=\"float16\",\n",
    "\n",
    "    device_map=\"cuda\",\n",
    "\n",
    ")\n",
    "\n",
    "processor = LlavaOnevisionProcessor.from_pretrained(\n",
    "\n",
    "    \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n",
    "\n",
    ")\n",
    "\n",
    "processor.tokenizer.padding_side = (\n",
    "\n",
    "    \"left\"  # set to 'left' for generation and 'right' for training (default in 'right')\n",
    "\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment below if you want to use 7B model and load it in consumer hardware\n",
    "\n",
    "# Qunatizing model to 4bits will save memory up to 4 times\n",
    "\n",
    "# model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "\n",
    "#     \"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n",
    "\n",
    "#     quantization_config=quantization_config,\n",
    "\n",
    "#     device_map='auto'\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: False\n",
      "GPU Name: No GPU available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "# Get the name of the GPU\n",
    "gpu_name = torch.cuda.get_device_name(0) if gpu_available else \"No GPU available\"\n",
    "\n",
    "print(f\"GPU Available: {gpu_available}\")\n",
    "print(f\"GPU Name: {gpu_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# import requests\n",
    "\n",
    "model_id = \"llava-hf/llava-interleave-qwen-0.5b-hf\"\n",
    "pipe = pipeline(\"image-to-text\", model=model_id, device=0)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\n",
    "    \"C:\\\\Users\\\\madan\\\\Projects\\\\BlindGlasses\\\\LocalLLMs\\\\TestData\\\\testimage2.jpg\"\n",
    ")\n",
    "\n",
    "\n",
    "# Define a chat histiry and use apply_chat_template to get correctly formatted prompt\n",
    "\n",
    "\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\")\n",
    "\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"describe the image to a blind person in 50 words\",\n",
    "            },\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
    "\n",
    "\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLaVaQwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
